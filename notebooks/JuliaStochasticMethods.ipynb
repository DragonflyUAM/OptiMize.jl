{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Methods\n",
    "\n",
    "Employ a randomized strategy for exploring a design space to jump out of local minima and hopefully enter global minima.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noisy Descent \n",
    "\n",
    "$x^{k+1} = x^k + \\alpha * g^{k} + \\epsilon^{k}$\n",
    "\n",
    "This augments gradient descent with additive gaussian noise $\\epsilon$ \n",
    "\n",
    "## Mesh Adaptive \n",
    "\n",
    "Similar to noisy descent however it will use random spanning directions to search dictated by $+-1 / \\sqrt(\\alpha^k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulated Annealing\n",
    "\n",
    "Inspired by metallurgy (material is heated and cooled) uses a temperature term to control the amount of stochasticity used in the randomized search.\n",
    "\n",
    "1. Logarithmic Annealing (gauranteed but slow)\n",
    "\n",
    "$t^{k} = t^{1} * \\ln(2) / \\ln(k + 1)$\n",
    "\n",
    "2. Exponential Annealing (faster)\n",
    "\n",
    "$t^{k+1} = \\gamma * t^{k}$ where $\\gamma = 0...1$\n",
    "\n",
    "3. Fast Annealing \n",
    "\n",
    "$t^{k} = t^{1} / k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "simulated_annealing (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function simulated_annealing(f, x, T, t, k_max)\n",
    "    y = f(x)\n",
    "    x_best, y_best = x, y\n",
    "    for k in 1:k_max\n",
    "        # Expand search space by temperature\n",
    "        x_new = x + rand(T)\n",
    "\n",
    "        # Evaluate new point\n",
    "        y_new = f(x_new)\n",
    "\n",
    "        # Calculate the difference between the new and old point\n",
    "        delta_y = y_new - y\n",
    "\n",
    "        # If the new point is better or change is small enough, accept it\n",
    "        if delta_y <= 0 || rand() < exp(-delta_y / t(k))\n",
    "            x, y = x_new, y_new\n",
    "        end\n",
    "\n",
    "        # If the new point is the best so far, update the best point\n",
    "        if y_new < y_best\n",
    "            x_best, y_best = x_new, y_new\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return x_best, y_best\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [15.0, 15.0], y = 19.00425863264272\n"
     ]
    }
   ],
   "source": [
    "ackey = (x) -> -20 * exp(-0.2 * sqrt(0.5 * (x[1]^2 + x[2]^2))) - exp(0.5 * (cos(2 * π * x[1]) + cos(2 * π * x[2]))) + 20 + exp(1)\n",
    "x0 = [15., 15.]\n",
    "T = 2\n",
    "t(k) = 1 / k\n",
    "\n",
    "x, y = simulated_annealing(ackey, x0, T, t, 100)\n",
    "println(\"x = $x, y = $y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can see 19 isn't the minimum of an Ackey function.. however making the simulated annealing to be more adaptive and larger sigma of energy will help the algorithm to capture the global minimum... but lets see how another method will work below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Entropy Method\n",
    "\n",
    "Very similar to KL divergence where we sample from a proposal distribution to minimize the delta between proposal and target with sampling best samples (e.g., find maximum liklihood)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cross_entropy_method (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using Distributions\n",
    "\n",
    "function cross_entropy_method(f, P, k_max; m=100, m_elite=10)\n",
    "    for k in 1:k_max\n",
    "        # Sample from the distribution (nxm matrix where n is the dimension of the input space P and m is the number of samples)\n",
    "        samples = rand(P, m)\n",
    "        # Evaluate the samples and sort them\n",
    "        order = sortperm([f(samples[:, i]) for i in 1:m])\n",
    "        # Update the distribution based on the elite samples\n",
    "        P = fit(typeof(P), samples[:, order[1:m_elite]])\n",
    "    end\n",
    "    return P\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P.μ = [-7.818623595598141e-8, 2.253462157183674e-7]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2-element Vector{Float64}:\n",
       " -7.818623595598141e-8\n",
       "  2.253462157183674e-7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import Random: seed!\n",
    "import LinearAlgebra: norm\n",
    "\n",
    "seed!(42)\n",
    "\n",
    "μ = [0.5, 1.5]\n",
    "Σ = [1.0 0.2; 0.2 2.0]\n",
    "P = MvNormal(μ, Σ)\n",
    "k_max = 10\n",
    "\n",
    "P = cross_entropy_method(ackey, P, k_max)\n",
    "@show P.μ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out this method works very will fiting a proposal distribution to minimize the the objective function. Below we can see how this can be done more efficiently using gradient descent of the log liklihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Evolution Strategies\n",
    "\n",
    "Similar to Cross-Entropy Method, natural evolution strategy seeks to minimize a function from a proposal distribution parameterized by $\\theta$. We seek to minimize the expectation of a function parameteried by $\\theta\\$ instead of taking fitting elite samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO calcualte the gradient of the log-likelihood\n",
    "\n",
    "function natural_evolution_strategy(f, θ, k_max; m=100, α=0.01)\n",
    "    for k in 1:k_max\n",
    "        # Sample from the distribution\n",
    "        samples = [rand(θ, length(θ)) for i in 1:m]\n",
    "        # Evaluate the samples, calculate the log-likelihood, and integrate them by taking the average and applying the learning rate to update theta\n",
    "        fx = [f(x) for x in samples]\n",
    "        logp = [∇logp(x, θ) for x in samples]\n",
    "        θ -= α * sum(fx * logp) / m\n",
    "    end\n",
    "    return θ\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "θ = natural_evolution_strategy(ackey, μ, 100)\n",
    "@show θ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.0",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
