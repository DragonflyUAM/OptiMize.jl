{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Descent\n",
    "\n",
    "In the general sense, local descent methods provide an optimization framework for solving multivariate functions to converge to some local minima based on some convergence criterion (i.e., gradient descent).\n",
    "\n",
    "A common approach is to incrementally improve the solution by taking steps in the descent direction (e.g., negative of gradient) of the function at the current point until a terminal or convergence condition is reached.\n",
    "\n",
    "$x_{k+t} = x_k + α_k * d_k$\n",
    "\n",
    "where $x_k$ is the current point, $α_k$ is the step size, and $d_k$ is the descent direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line Search\n",
    "\n",
    "In this first form of local descent, we will use a line search to find the optimal step size $α_k$ followed by computing the next point $x_{k+t}$.\n",
    "\n",
    "$minimize \\space f(x_k + α_k * d_k)$\n",
    "\n",
    "note this is still a univariate function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line Search via Backtracking\n",
    "\n",
    "In this type of algorithm we seek to find the optimum α for a direcitonal descent method.\n",
    "        \n",
    "In order to do this we need to define the two Wolfe Conditions:\n",
    "\n",
    "1. Sufficient decrease in α\n",
    "    1. $f(x_k) <= f(x) + α * t * dot(df(x), d)$ where $x$ is the design point, $d$ is the direction, and $t$ is the step size\n",
    "2. Curvature condition\n",
    "    1. $|∇d(k) * f(x(k+1))| ≤ −σ * dot(∇d(k), f(x(k)))$ where $σ$ is a shallowness indicator typically between $β < σ < 1$\n",
    "\n",
    "These two conditions form the strong backtracking algorithm. In the below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "using LinearAlgebra\n",
    "using OptiMize: complex_step, plot_min_of_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "line_search_backtracking (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function line_search_backtracking(f, df, x, d, n; α=0.5, β=0.8, σ=0.9)\n",
    "    t = 1.0\n",
    "    \n",
    "    for i in 1:n\n",
    "        # Calculate x_k update\n",
    "        x_k = x + t * d\n",
    "\n",
    "        # Caclulate both wolfe conditions\n",
    "        armijo_condition = f(x_k) <= f(x) .+ α * t * dot(df(x), d)\n",
    "        curvature_condition = abs(dot(df(x_k), d)) <= -σ * dot(df(x), d)\n",
    "        \n",
    "        # Terminate optimization of alpha if wolfe conditions are met\n",
    "        if armijo_condition && curvature_condition\n",
    "            return t\n",
    "        end\n",
    "        \n",
    "        # Update step size by beta\n",
    "        t *= β\n",
    "    end\n",
    "    \n",
    "    return t\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal step size: 0.40960000000000013\n"
     ]
    }
   ],
   "source": [
    "# Define the function and its derivative\n",
    "f(x) = x.^2\n",
    "df(x) = complex_step(f, x)\n",
    "\n",
    "# Initial guess and search direction\n",
    "x0 = [4.]\n",
    "d = -df(x0) # Search direction\n",
    "\n",
    "# Perform the line search\n",
    "step_size = line_search_backtracking(f, df, x0, d, 10)\n",
    "\n",
    "println(\"Optimal step size: $step_size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trust Region Methods\n",
    "\n",
    "One primary issue with direct descent is that they tend to either prematurely converge or overshoot. A way around this issue is to deploy trust regions which contract or expand the region of descent depending on how aligned the prediction was to the actual improvement.\n",
    "\n",
    "$minimize f\\hat(x_k)$ s.t. $||x - x_k|| \\le δ$\n",
    "\n",
    "1. where $x$ is our design point, $x_k$ is the next design point, and $δ$ is trust region's radius. This is also formulated as a constrained optimization problem.\n",
    "\n",
    "1. $δ$ is exapnded or contracted depending on the model's predictive performance.\n",
    "\n",
    "1. $η = actual / predicted = (f(x) - f(x_k)) \\div (f(x) - f\\hat(x_hat))$\n",
    "\n",
    "1. we'd like to then iteratively improve this performance by scaling δ based on performance η utilizing scale factor γ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trust_region_descent (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function trust_region_descent(f, df, d2f, x0, δ, n; η1=0.25, η2=0.5, γ1=0.5, γ2=2.0)\n",
    "    x = copy(x0)\n",
    "    \n",
    "    for i in 1:n\n",
    "        gradient = df(x)\n",
    "        hessian = d2f(x)\n",
    "        \n",
    "        # Solve the trust region subproblem: minimize m(s) = f(x) + gradient' * s + 0.5 * s' * hessian * s subject to ||s|| <= Δ\n",
    "        rhs = -gradient\n",
    "        lhs = hessian + δ * I  # Add Δ * I to the Hessian matrix for the trust region constraint\n",
    "        s = zeros(size(x))\n",
    "        s = lhs \\ rhs\n",
    "        \n",
    "        actual_reduction = f(x + s) - f(x)\n",
    "        predicted_reduction = -0.5 * dot(gradient, s) - 0.5 * dot(s, hessian * s)\n",
    "        rho = actual_reduction / predicted_reduction\n",
    "        \n",
    "        if rho < η1\n",
    "            δ *= γ1\n",
    "        elseif rho > η2\n",
    "            δ *= γ2\n",
    "        end\n",
    "        \n",
    "        x += s\n",
    "    end\n",
    "    \n",
    "    return x\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal solution: [3.412205993379098e-20, 5.1183089900686466e-20]\n"
     ]
    }
   ],
   "source": [
    "# Define the function to minimize, its gradient, and Hessian\n",
    "# TODO solve this with differentiate functions (update to differentiate mxn matrices)\n",
    "f(x) = x[1]^2 + x[2]^2\n",
    "df(x) = [2x[1], 2x[2]]\n",
    "d2f(x) = [2 0; 0 2]\n",
    "\n",
    "# Initial guess, trust region size, and tolerance\n",
    "x0 = [2.0, 3.0]\n",
    "Δ = 0.5\n",
    "\n",
    "# Run the trust region descent algorithm\n",
    "result = trust_region_descent(f, df, d2f, x0, Δ, 10)\n",
    "\n",
    "println(\"Optimal solution: $result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
